{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import cv2 as cv\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#Â for running DeepMatching\n",
    "import subprocess\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = './some-results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay(img_1, img_2):\n",
    "    \n",
    "    if len(img_1.shape) == 3:\n",
    "        overlay = np.zeros_like(img_1, dtype='uint8')\n",
    "    else:\n",
    "        overlay = np.zeros((img_1.shape[0], img_1.shape[1], 3), dtype='uint8')\n",
    "\n",
    "    overlay[:,:,0] = img_1\n",
    "    overlay[:,:,1] = img_2\n",
    "    \n",
    "    return overlay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating some test Image Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_view = 'KL11-E1DC'\n",
    "\n",
    "availible_ids = [int(file.split('.')[0])\n",
    "                 for file in os.listdir(f\"./data/real/{camera_view}/\") \n",
    "                 if file.endswith(\".png\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from random import shuffle\n",
    "import random\n",
    "\n",
    "random.seed(123)\n",
    "n = 500\n",
    "#n = 100\n",
    "img_pairs = list(itertools.combinations(availible_ids, 2))\n",
    "shuffle(img_pairs)\n",
    "img_pairs = img_pairs[:n]\n",
    "img_pairs[:3] # check this doesn't change!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check we don't have any duplicates\n",
    "for id_1, id_2 in img_pairs:\n",
    "    if id_1 == id_2:\n",
    "        print('oops, try again')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run DeepMatching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good idea to find out exactly how strict we can make these!\n",
    "max_scale_factor = 1.05\n",
    "max_rotation_angle = 2\n",
    "\n",
    "def deepmatch(img_id_1, img_id_2):\n",
    "    \"\"\"\n",
    "    Make a call to the DeepMatching algorithm and return the \n",
    "    resulting point pairs in a DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    # build command\n",
    "    cmd = f'./deepmatching/deepmatching {img_id_1} {img_id_2} '\n",
    "    cmd += f'-max_scale {max_scale_factor} '\n",
    "    cmd += f'-rot_range 0 {max_rotation_angle} '\n",
    "    cmd += '-resize 500 500 ' # smaller images are processed much faster!\n",
    "    #cmd += '-nt 16' # number of threads for faster completion\n",
    "    \n",
    "    # split so Popen can do it's thing correctly\n",
    "    cmd = cmd.split()\n",
    "    \n",
    "    # open the subprocess\n",
    "    process = subprocess.Popen(cmd,\n",
    "                               stdout=subprocess.PIPE, \n",
    "                               stderr=subprocess.PIPE)\n",
    "\n",
    "    # wait for the process to terminate and decode\n",
    "    out, err = process.communicate()\n",
    "    out = StringIO(out.decode(\"utf-8\"))\n",
    "    df = pd.read_csv(out, sep=\" \", header=None, \n",
    "                     names=['x_1','y_1','x_2','y_2','score','index'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def deepmatch_many(img_pairs, data_path):\n",
    "    \"\"\"\n",
    "    Run DM for each image pair in a set of image pairs.\n",
    "    Write point pairs to csv.\n",
    "    \"\"\"\n",
    "    \n",
    "    for id_1, id_2 in img_pairs:\n",
    "\n",
    "        img1_path = os.path.join(data_path, str(id_1) + '.png')\n",
    "        img2_path = os.path.join(data_path, str(id_2) + '.png')\n",
    "        \n",
    "        # deepmatch\n",
    "        point_pairs = deepmatch(img1_path, img2_path)\n",
    "        \n",
    "        # write results\n",
    "        outpath = os.path.join('./some-results/', pp_batch, f'{id_1}_to_{id_2}.csv')\n",
    "        point_pairs.to_csv(outpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name of the preprocessing stratergy to run DM on\n",
    "pp_batch = 'unpp'\n",
    "\n",
    "# build the path of where the preprocessed images are saved\n",
    "data_path = os.path.join('./some-results/data/', pp_batch)\n",
    "\n",
    "deepmatch_many(img_pairs, data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring\n",
    "\n",
    "Kept some plots of edge detection here from when looking at early ways to score. An original idea was to only score pixels that were near a detected edge (as this is where red/green fringes will be found). This might be useful in the future. Note also that the Laplacian edge detection with a large radius parameter is actually not as bad as we thought in the first week of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair = img_pairs[63]\n",
    "\n",
    "pp_path = 'clustered_norm'\n",
    "\n",
    "img_1 = cv.imread(os.path.join(base_path, 'data', pp_path, str(pair[0]) + '.png'), cv.IMREAD_GRAYSCALE)\n",
    "img_2 = cv.imread(os.path.join(base_path, 'data', pp_path, str(pair[1]) + '.png'), cv.IMREAD_GRAYSCALE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "for i, r in enumerate(np.linspace(5,200,9).astype(int)):\n",
    "    plt.subplot(331+i)\n",
    "    diff = np.abs(img_1.astype(int) - cv.blur(img_1, (r,r)).astype(int))\n",
    "    plt.imshow(diff)\n",
    "    plt.title(f'mean filter of size {r}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "for i, r in enumerate(np.linspace(5,200,9).astype(int)):\n",
    "    if r % 2 == 0:\n",
    "        r += 1\n",
    "    plt.subplot(331+i)\n",
    "    diff = np.abs(img_1.astype(int) - cv.GaussianBlur(img_1, (r,r), 0).astype(int))\n",
    "    plt.imshow(diff)\n",
    "    plt.title(f'Gaussian filter of size {r}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "for i, r in enumerate(np.linspace(5,31,9).astype(int)):\n",
    "    if r % 2 == 0:\n",
    "        r += 1\n",
    "    plt.subplot(331+i)\n",
    "    diff = np.abs(cv.Laplacian(img_1, cv.CV_64F, ksize=r).astype(int))\n",
    "    plt.imshow(diff)\n",
    "    plt.title(f'Laplacian of size {r}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 200\n",
    "img = img_1[300:450, 210:310]\n",
    "\n",
    "dst = cv.blur(img, (r, r))\n",
    "isolated = np.abs(img.astype(int) - dst.astype(int))\n",
    "\n",
    "plt.imshow(isolated)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we could only score pixels above a certain threshold here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diff Diff Method\n",
    "\n",
    "We need two images to be matched $l_1$ and $l_2$, and a transformation of the first image $l_1'$. We assess the score of the tranformation using \n",
    "$$\n",
    "|l_1 - l_2| - |l_1' - l_2|\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(img, b=5):\n",
    "    rows, cols = img.shape\n",
    "    img = img.copy()\n",
    "    img[0:b, :] = 0\n",
    "    img[rows-b:rows, :] = 0\n",
    "    img[:, 0:b] = 0\n",
    "    img[:, cols-b:cols] = 0\n",
    "\n",
    "    return img\n",
    "\n",
    "def get_diff_diff(img_1, img_1_h, img_2, crop=0, tol=50, flat_filtered=True):\n",
    "    if crop != 0:\n",
    "        img_1 = crop_image(img_1, crop)\n",
    "        img_1_h = crop_image(img_1_h, crop)\n",
    "        img_2 = crop_image(img_2, crop)\n",
    "    \n",
    "    diff_before = np.abs(img_1.astype(int) - img_2.astype(int))\n",
    "    diff_after = np.abs(img_1_h.astype(int) - img_2.astype(int))\n",
    "    diff_diff = diff_before - diff_after\n",
    "    if flat_filtered:\n",
    "        diff_diff_filtered = diff_diff[np.abs(diff_diff) > tol]\n",
    "        return diff_diff_filtered\n",
    "    else:\n",
    "        return diff_diff\n",
    "\n",
    "    \n",
    "\n",
    "def show_diff_diff(img_1, img_1_h, img_2, crop=0, tol=50):\n",
    "    if crop != 0:\n",
    "        img_1 = crop_image(img_1, crop)\n",
    "        img_1_h = crop_image(img_1_h, crop)\n",
    "        img_2 = crop_image(img_2, crop)\n",
    "        \n",
    "    dd = get_diff_diff(img_1, img_1_h, img_2, crop, tol, flat_filtered=False)\n",
    "    overl = overlay(img_1_h, img_2)\n",
    "    overl[np.where(dd > tol)] = 255\n",
    "    overl[np.where(dd < -tol)] = 0\n",
    "    plt.imshow(overl, vmin=0, vmax=255)\n",
    "    \n",
    "def diff_diff_hist(img_1, img_1_h, img_2, tol=50, crop=0):\n",
    "    if crop != 0:\n",
    "        img_1 = crop_image(img_1, crop)\n",
    "        img_1_h = crop_image(img_1_h, crop)\n",
    "        img_2 = crop_image(img_2, crop)\n",
    "        \n",
    "    counts, _, _ = plt.hist(get_diff_diff(img_1, img_1_h, img_2, crop, tol), bins=255//15, range=(-255,255) )\n",
    "    plt.plot([tol-5,tol-5], [0, max(counts) * 1.1], linewidth=5, color='black')\n",
    "    plt.plot([-tol+5,-tol+5], [0, max(counts) * 1.1], linewidth=5, color='black')\n",
    "    \n",
    "    plt.xlabel(\"$|l_1 - l_2| - |l_1' - l_2|$\")\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()\n",
    "    \n",
    "def dd_score(img_1, img_1_h, img_2, tol=50, crop=0):\n",
    "    \"\"\"\n",
    "    TODO: if less than X are above tolerance, then return that \n",
    "    the image has not substantially changed. This is a bit tricky \n",
    "    ratio becomes unstable for low counts of changed pixel locations,\n",
    "    but if the images are already aligned, then the score should not\n",
    "    pick up anything.\n",
    "    So: how to distingush between: bad alignment but transformation\n",
    "    has changed nothing and good initial alignment so transformation\n",
    "    doesn't need to change anything. (can probably do by checking if\n",
    "    the score is at a minimum in homography space)\n",
    "    \"\"\"\n",
    "    if crop != 0:\n",
    "        img_1 = crop_image(img_1, crop)\n",
    "        img_1_h = crop_image(img_1_h, crop)\n",
    "        img_2 = crop_image(img_2, crop)\n",
    "        \n",
    "    dd = get_diff_diff(img_1, img_1_h, img_2, crop, tol)\n",
    "    pos = np.where(dd > tol)[0]\n",
    "    neg = np.where(dd < -tol)[0]\n",
    "    if len(dd) == 0:\n",
    "        return 0.5, 0\n",
    "    #\n",
    "    # instead of weighting, could just return the ratio\n",
    "    # and an additional 'confidence' score based on the\n",
    "    # number of points and/or sublock agreements \n",
    "    #\n",
    "    #return len(pos) / len(neg) * np.log(len(dd))*2\n",
    "    return len(pos) / len(dd), len(dd)\n",
    "    #return len(pos) / len(neg) * len(dd)**0.1\n",
    "    # ratio and count separately\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair = img_pairs[63]\n",
    "\n",
    "pp_path = 'clustered_norm'\n",
    "\n",
    "img_1 = cv.imread(os.path.join(base_path, 'data', pp_path, str(pair[0]) + '.png'), cv.IMREAD_GRAYSCALE)\n",
    "img_2 = cv.imread(os.path.join(base_path, 'data', pp_path, str(pair[1]) + '.png'), cv.IMREAD_GRAYSCALE)\n",
    "\n",
    "point_pairs = pd.read_csv(os.path.join(base_path, pp_path, str(pair[0]) + '_to_' + str(pair[1]) + '.csv'),\n",
    "                         index_col=0)\n",
    "\n",
    "before = point_pairs[['x_1','y_1']].values\n",
    "after = point_pairs[['x_2','y_2']].values\n",
    "\n",
    "h_matrix, mask = cv.findHomography(before, after, method=cv.RANSAC)\n",
    "img_1_h = cv.warpPerspective(img_1, h_matrix, dsize=img_1.shape)\n",
    "\n",
    "plt.figure(1, figsize=(17,9))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(f'Overlaid Images - {pair[0]} and {pair[1]}')\n",
    "plt.imshow(overlay(img_1, img_2))\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(f'DM Transformed Overlay - {pair[0]} and {pair[1]}')\n",
    "plt.imshow(overlay(img_1_h, img_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the above matching, we can see that the tile section in the bottom-centre of the image are much improved, while other regions such as the bottom right are mostly unchanged. Let's plot on top of this transformed image the regions our scoring system is detecting change. We see that after filtering out some noise, we get very sensible results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,16))\n",
    "\n",
    "for i in range(9):\n",
    "    plt.subplot(331 + i)\n",
    "    show_diff_diff(img_1, img_1_h, img_2, crop=5, tol=i*10+10)\n",
    "    plt.title(f'tol = {i*10 + 10}')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_diff_hist(img_1, img_1_h, img_2, 50, crop=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_score(img_1, img_1_h, img_2, crop=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram shows that there are more locations in the transformed pair that have a smaller distance between pixel intensities. i.e. the images are more similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROI - Tiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can select a region where we expect to see a lot of this behaviour, to remove noise from other parts of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_1_r1 = img_1[300:450, 190:310]\n",
    "img_2_r1 = img_2[300:450, 190:310]\n",
    "img_1_h_r1 = img_1_h[300:450, 190:310]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the differences of the two images at each position to get information about the red/green fringes. Do this after transforming the image as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_diff_hist(img_1_r1, img_1_h_r1, img_2_r1, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The skew here is evidence that the fringes have gone away"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(9,6))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.gca().get_xaxis().set_visible(False)\n",
    "plt.gca().get_yaxis().set_visible(False)\n",
    "\n",
    "plt.title(f'Overlaid Images')\n",
    "plt.imshow(overlay(img_1[300:450, 190:310], img_2[300:450, 190:310]))\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.gca().get_xaxis().set_visible(False)\n",
    "plt.gca().get_yaxis().set_visible(False)\n",
    "\n",
    "plt.title(f'Transformed Overlay')\n",
    "plt.imshow(overlay(img_1_h[300:450, 190:310], img_2[300:450, 190:310]))\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.gca().get_xaxis().set_visible(False)\n",
    "plt.gca().get_yaxis().set_visible(False)\n",
    "\n",
    "plt.title(f'Scored Overlay')\n",
    "\n",
    "dd = get_diff_diff(img_1_r1, img_1_h_r1, img_2_r1, 0, 50, flat_filtered=False)\n",
    "overl = overlay(img_1_h_r1, img_2_r1)\n",
    "overl[np.where(dd > 50)] = 255\n",
    "overl[np.where(dd < -50)] = 0\n",
    "plt.imshow(overl, vmin=0, vmax=255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_score(img_1_r1, img_1_h_r1, img_2_r1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regions of interest will also be useful to detect regions of the image that have been badly scored. Note instead of ROIs you could just split each image into a 2x2 grid and then score each subblock. Then if one subblock has a score which is very different you could ignore it as an outlier. An example of the kind of situaion where this is needed is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROI - Top right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_1_r2 = img_1[0:50, 450:500]\n",
    "img_2_r2 = img_2[0:50, 450:500]\n",
    "img_1_h_r2 = img_1_h[0:50, 450:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_diff_hist(img_1_r2, img_1_h_r2, img_2_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_diff_diff(img_1_r2, img_1_h_r2, img_2_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(17,9))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(f'Overlaid Images - {pair[0]} and {pair[1]}')\n",
    "plt.imshow(overlay(img_1_r2, img_2_r2))\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(f'Transformed Overlay - {pair[0]} and {pair[1]}')\n",
    "plt.imshow(overlay(img_1_h_r2, img_2_r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_score(img_1_r2, img_1_h_r2, img_2_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method is very accurate as demonstrated here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROI - Side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_1_r3 = img_1[200:490, 100:175]\n",
    "img_2_r3 = img_2[200:490, 100:175]\n",
    "img_1_h_r3 = img_1_h[200:490, 100:175]\n",
    "\n",
    "plt.imshow(img_1_r3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_diff_hist(img_1_r3, img_1_h_r3, img_2_r3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_diff_diff(img_1_r3, img_1_h_r3, img_2_r3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_score(img_1_r3, img_1_h_r3, img_2_r3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translate img_1 in the wrong direction and check the diff diff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows,cols = img_1.shape\n",
    "\n",
    "dx = 10\n",
    "dy = 10\n",
    "\n",
    "M = np.float32([[1,0,dx],[0,1,dy]])\n",
    "img_1_t = cv.warpAffine(img_1, M, (cols,rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(17,9))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.gca().get_xaxis().set_visible(False)\n",
    "plt.gca().get_yaxis().set_visible(False)\n",
    "plt.title(f'Overlaid Images - {pair[0]} and {pair[1]}', fontsize=15)\n",
    "plt.imshow(overlay(img_1, img_2))\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.gca().get_xaxis().set_visible(False)\n",
    "plt.gca().get_yaxis().set_visible(False)\n",
    "plt.title(f'Transformed Overlay - {pair[0]} and {pair[1]}', fontsize=15)\n",
    "plt.imshow(overlay(img_1_t, img_2))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_diff_hist(img_1, img_1_t, img_2, crop=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "show_diff_diff(img_1, img_1_t, img_2, crop=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_score(img_1, img_1_t, img_2, crop=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the score is low. Looking at some of the specific regions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_diff_hist(img_1[300:450, 210:310], img_1_t[300:450, 210:310], img_2[300:450, 210:310])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_diff_diff(img_1[300:450, 210:310], img_1_t[300:450, 210:310], img_2[300:450, 210:310])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brute force alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the ddscore as a function of translation distance. Originally just wanted to show that increasing translation distance lead to a decreasing score, but actually from the peak here we can find the best alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = []\n",
    "ys = []\n",
    "dists = []\n",
    "scores = []\n",
    "\n",
    "for dx in range(10):\n",
    "    for dy in range(1,10):\n",
    "        xs.append(dx)\n",
    "        ys.append(dy)\n",
    "        \n",
    "        dist = (dx**2 + dy**2)**0.5\n",
    "        dists.append(dist)\n",
    "\n",
    "        M = np.float32([[1,0,dx],[0,1,dy]])\n",
    "        img_1_t = cv.warpAffine(img_1, M, (cols,rows))\n",
    "\n",
    "        score = dd_score(img_1, \n",
    "                         img_1_t,\n",
    "                         img_2, crop=15, tol=50)[0]\n",
    "        \n",
    "        scores.append(score)\n",
    "\n",
    "plt.scatter(dists, scores)\n",
    "plt.xlabel('translation distance', fontsize=14)\n",
    "plt.ylabel('dd_score', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general the trend of decrasing score with increasing distance is a good one. What about looking at that peak though?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_index = np.argmax(scores)\n",
    "x_best, y_best = xs[best_index], ys[best_index]\n",
    "print(x_best, y_best)\n",
    "\n",
    "M = np.float32([[1,0,x_best],[0,1,y_best]])\n",
    "img_1_t_best = cv.warpAffine(img_1, M, (cols,rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(14,9))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.gca().get_xaxis().set_visible(False)\n",
    "plt.gca().get_yaxis().set_visible(False)\n",
    "plt.title(f'Overlaid Images - {pair[0]} and {pair[1]}', fontsize=15)\n",
    "plt.imshow(overlay(img_1, img_2))\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.gca().get_xaxis().set_visible(False)\n",
    "plt.gca().get_yaxis().set_visible(False)\n",
    "plt.title(f'Brute Force Translation - DD Optimised', fontsize=15)\n",
    "plt.imshow(overlay(img_1_t_best, img_2))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimising the DD score here actually gets us a better match than from deepmatching! This is not usually the case, and seems to have been lucky. But we could still entertain the idea of optimising the full homography matrix using the ddscore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test New Image\n",
    "\n",
    "We run into a fail case with the low contrast images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair = img_pairs[2]\n",
    "\n",
    "pp_path = 'ds_bl_nlm'\n",
    "\n",
    "img_1 = cv.imread(os.path.join(base_path, 'data', pp_path, str(pair[0]) + '.png'), cv.IMREAD_GRAYSCALE)\n",
    "img_2 = cv.imread(os.path.join(base_path, 'data', pp_path, str(pair[1]) + '.png'), cv.IMREAD_GRAYSCALE)\n",
    "\n",
    "point_pairs = pd.read_csv(os.path.join(base_path, pp_path, str(pair[0]) + '_to_' + str(pair[1]) + '.csv'),\n",
    "                         index_col=0)\n",
    "\n",
    "before = point_pairs[['x_1','y_1']].values\n",
    "after = point_pairs[['x_2','y_2']].values\n",
    "\n",
    "h_matrix, mask = cv.findHomography(before, after, method=cv.RANSAC)\n",
    "img_1_h = cv.warpPerspective(img_1, h_matrix, dsize=img_1.shape)\n",
    "\n",
    "plt.figure(1, figsize=(11,9))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(f'Overlaid Images - {pair[0]} and {pair[1]}')\n",
    "plt.imshow(overlay(img_1, img_2)[0:100,400:500])\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(f'Transformed Overlay - {pair[0]} and {pair[1]}')\n",
    "plt.imshow(overlay(img_1_h, img_2)[0:100,400:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here the match in the top right is much improved. However, because of the residual red glow from img_1 which has moved into the dark region of the image, we detect a lot of badly scored pixels. An alternative to subblock scoring would be to treat the negatives separately, or just ignore them (as they can happen in this type of circumstance).\n",
    "\n",
    "You can't really prevent this through clever choice of tolerance either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,16))\n",
    "\n",
    "tols = np.linspace(10,30,9).astype(int)\n",
    "\n",
    "for i in range(9):\n",
    "    plt.subplot(331 + i)\n",
    "    show_diff_diff(img_1, img_1_h, img_2, crop=5, tol=tols[i])\n",
    "    plt.title(f'tol = {tols[i]}')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fail Case\n",
    "\n",
    "Example of classic fail case of the scoring algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair = img_pairs[6]\n",
    "\n",
    "pp_path = 'ds_bl_nlm'\n",
    "\n",
    "img_1 = cv.imread(os.path.join(base_path, 'data', pp_path, str(pair[0]) + '.png'), cv.IMREAD_GRAYSCALE)\n",
    "img_2 = cv.imread(os.path.join(base_path, 'data', pp_path, str(pair[1]) + '.png'), cv.IMREAD_GRAYSCALE)\n",
    "\n",
    "point_pairs = pd.read_csv(os.path.join(base_path, pp_path, str(pair[0]) + '_to_' + str(pair[1]) + '.csv'),\n",
    "                         index_col=0)\n",
    "\n",
    "before = point_pairs[['x_1','y_1']].values\n",
    "after = point_pairs[['x_2','y_2']].values\n",
    "\n",
    "h_matrix, mask = cv.findHomography(before, after, method=cv.RANSAC)\n",
    "img_1_h = cv.warpPerspective(img_1, h_matrix, dsize=img_1.shape)\n",
    "\n",
    "plt.figure(1, figsize=(14,9))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(f'Overlaid Images - {pair[0]} and {pair[1]}')\n",
    "plt.imshow(overlay(img_1, img_2)[400:500,400:500], vmin=0, vmax=255)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(f'Transformed Overlay - {pair[0]} and {pair[1]}')\n",
    "plt.imshow(overlay(img_1_h, img_2)[400:500,400:500], vmin=0, vmax=255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we can see that the bottom right of the image has clearly improved accross the central diagonal curve (the green band on the left is small than the red band on the right). The problem is, in this case, and in this region, img_1 is substantially more bright than image 2, by about 20 pixels. (in the very bottom right, we have a red fringe replaced with a green fringe - essentially no change. the algorithm successfully handles this case). We can note the difference from the overall reddy tinge to the overlay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img_1[400:500,400:500].mean())\n",
    "print(img_2[400:500,400:500].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(10,9))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(f'img_1 - brighter overall')\n",
    "plt.imshow(img_1[400:500,400:500], vmin=0, vmax=255)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(f'img_2 - darker overall')\n",
    "plt.imshow(img_2[400:500,400:500], vmin=0, vmax=255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The specific combination of a bright fringe in the top right of img_1 and the very dark patch in the middle right on img_2 means that when that bright patch slightly intrudes on teh dark patch we get a big swing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that for any choice of tolerance, we detect this region as having gotten worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "tols = np.linspace(10,40,9).astype(int)\n",
    "\n",
    "for i in range(9):\n",
    "    plt.subplot(331 + i)\n",
    "    show_diff_diff(img_1[400:500,400:500], img_1_h[400:500,400:500], img_2[400:500,400:500], crop=0, tol=tols[i])\n",
    "    plt.title(f'tol = {tols[i]}')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After compensating for the lack of luminance in img_2, we restore the correct behaviour of the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_2[425:475,425:475] += 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "tols = np.linspace(10,40,9).astype(int)\n",
    "\n",
    "for i in range(9):\n",
    "    plt.subplot(331 + i)\n",
    "    show_diff_diff(img_1[400:500,400:500], img_1_h[400:500,400:500], img_2[400:500,400:500], crop=0, tol=tols[i])\n",
    "    plt.title(f'tol = {tols[i]}')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this mean:\n",
    "- it may be preferrable to have highly contrast corrected images, as these might have less of these absolute brightness differences (haven't confirmed this yet but it makes sense).\n",
    "- it may a good idea to score the image in subblocks, so that a) we can check for subblock consistency (i.e. this bottom right subblock would stick out when all the other subblocks would be unchanged or have improved) AND/OR b) check for and correct all sizeable differences in meanillumination between subblocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like correcting the average illumination in the subblock is feasbile: here is another very extreme example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair = img_pairs[8]\n",
    "\n",
    "pp_path = 'ds_bl_nlm'\n",
    "\n",
    "img_1 = cv.imread(os.path.join(base_path, 'data', pp_path, str(pair[0]) + '.png'), cv.IMREAD_GRAYSCALE)\n",
    "img_2 = cv.imread(os.path.join(base_path, 'data', pp_path, str(pair[1]) + '.png'), cv.IMREAD_GRAYSCALE)\n",
    "\n",
    "point_pairs = pd.read_csv(os.path.join(base_path, pp_path, str(pair[0]) + '_to_' + str(pair[1]) + '.csv'),\n",
    "                         index_col=0)\n",
    "\n",
    "before = point_pairs[['x_1','y_1']].values\n",
    "after = point_pairs[['x_2','y_2']].values\n",
    "\n",
    "h_matrix, mask = cv.findHomography(before, after, method=cv.RANSAC)\n",
    "img_1_h = cv.warpPerspective(img_1, h_matrix, dsize=img_1.shape)\n",
    "\n",
    "plt.figure(1, figsize=(14,9))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(f'Overlaid Images - {pair[0]} and {pair[1]}')\n",
    "plt.imshow(overlay(img_1, img_2), vmin=0, vmax=255)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(f'Transformed Overlay - {pair[0]} and {pair[1]}')\n",
    "plt.imshow(overlay(img_1_h, img_2), vmin=0, vmax=255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's obvious from the green hue in the bottom right we have crazy mismatched brightness in that region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_1[400:500,400:500].mean() - img_2[400:500,400:500].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(14,9))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(f'Overlaid Images - {pair[0]} and {pair[1]}')\n",
    "plt.imshow(overlay(img_1, img_2)[400:500,400:500], vmin=0, vmax=255)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(f'Transformed Overlay - {pair[0]} and {pair[1]}')\n",
    "plt.imshow(overlay(img_1_h, img_2)[400:500,400:500], vmin=0, vmax=255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zooming in we don't notice any obvious difference that the homography has made. However the score picks up a lot of signal! The nice thing is we show that it actually should be detecting SOMETHING here, it just gets confused by the relative illumination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "tols = np.linspace(5,20,9).astype(int)\n",
    "\n",
    "for i in range(9):\n",
    "    plt.subplot(331 + i)\n",
    "    show_diff_diff(img_1[400:500,400:500], img_1_h[400:500,400:500], img_2[400:500,400:500], crop=0, tol=tols[i])\n",
    "    plt.title(f'tol = {tols[i]}')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's correct that change in luminance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_2[300:500,300:500] -=100\n",
    "\n",
    "\n",
    "plt.figure(1, figsize=(17,9))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(f'Overlaid Images - {pair[0]} and {pair[1]}')\n",
    "plt.imshow(overlay(img_1, img_2)[300:500,300:500], vmin=0, vmax=255)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(f'Transformed Overlay - {pair[0]} and {pair[1]}')\n",
    "plt.imshow(overlay(img_1_h, img_2)[300:500,300:500], vmin=0, vmax=255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's now mch more obvious that the homography has actually gotten worse from the (still quite faint) red  fringes in the right hand image. After applying that correction we see expected behaviour from the scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,16))\n",
    "\n",
    "tols = np.linspace(5,20,9).astype(int)\n",
    "\n",
    "for i in range(9):\n",
    "    plt.subplot(331 + i)\n",
    "    show_diff_diff(img_1, img_1_h, img_2, crop=5, tol=tols[i])\n",
    "    plt.title(f'tol = {tols[i]}')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A New Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair = img_pairs[12]\n",
    "\n",
    "pp_path = 'clustered_norm'\n",
    "\n",
    "img_1 = cv.imread(os.path.join(base_path, 'data', pp_path, str(pair[0]) + '.png'), cv.IMREAD_GRAYSCALE)\n",
    "img_2 = cv.imread(os.path.join(base_path, 'data', pp_path, str(pair[1]) + '.png'), cv.IMREAD_GRAYSCALE)\n",
    "\n",
    "point_pairs = pd.read_csv(os.path.join(base_path, pp_path, str(pair[0]) + '_to_' + str(pair[1]) + '.csv'),\n",
    "                         index_col=0)\n",
    "\n",
    "before = point_pairs[['x_1','y_1']].values\n",
    "after = point_pairs[['x_2','y_2']].values\n",
    "\n",
    "h_matrix, mask = cv.findHomography(before, after, method=cv.RANSAC)\n",
    "img_1_h = cv.warpPerspective(img_1, h_matrix, dsize=img_1.shape)\n",
    "\n",
    "plt.figure(1, figsize=(17,9))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(f'Overlaid Images - {pair[0]} and {pair[1]}')\n",
    "plt.imshow(overlay(img_1, img_2))\n",
    "\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(f'Transformed Overlay - {pair[0]} and {pair[1]}')\n",
    "plt.imshow(overlay(img_1_h, img_2))\n",
    "plt.savefig(os.path.join(base_path, 'homographies', pp_path, str(pair[0]) + '_to_' + str(pair[1]) + '.png'),\n",
    "           dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_diff_hist(img_1, img_1_h, img_2, crop=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "show_diff_diff(img_1, img_1_h, img_2, crop=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are picking up exactly the right things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_score(img_1, img_1_h, img_2, crop=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brute_force_tf(img_1, img_2, tol=50):\n",
    "    \n",
    "    rows, cols = img_1.shape\n",
    "    x_min, y_min = int(rows*0.05), int(cols*0.05)\n",
    "    x_max, y_max = int(rows*0.95), int(cols*0.95)\n",
    "\n",
    "    xs = []\n",
    "    ys = []\n",
    "    dists = []\n",
    "    scores = []\n",
    "\n",
    "    for dx in range(15):\n",
    "        for dy in range(1,15):\n",
    "            xs.append(dx)\n",
    "            ys.append(dy)\n",
    "\n",
    "            dist = (dx**2 + dy**2)**0.5\n",
    "            dists.append(dist)\n",
    "\n",
    "            M = np.float32([[1,0,dx],[0,1,dy]])\n",
    "            img_1_t = cv.warpAffine(img_1, M, (cols,rows))\n",
    "\n",
    "            score = dd_score(img_1[y_min:y_max, x_min:x_max], \n",
    "                             img_1_t[y_min:y_max, x_min:x_max],\n",
    "                             img_2[y_min:y_max, x_min:x_max], tol=tol)\n",
    "\n",
    "            scores.append(score)\n",
    "            \n",
    "    return xs, ys, dists, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys, dists, scores = brute_force_tf(img_1, img_2, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(dists, scores)\n",
    "plt.xlabel('translation distance')\n",
    "plt.ylabel('dd_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_index = np.argmax(scores)\n",
    "x_best, y_best = xs[best_index], ys[best_index]\n",
    "print(x_best, y_best)\n",
    "\n",
    "M = np.float32([[1,0,x_best],[0,1,y_best]])\n",
    "img_1_t_best = cv.warpAffine(img_1, M, (cols,rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(17,9))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(f'Overlaid Images - {pair[0]} and {pair[1]}')\n",
    "plt.imshow(overlay(img_1, img_2))\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(f'Brute Force Translation')\n",
    "plt.imshow(overlay(img_1_t_best, img_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "show_diff_diff(img_1, img_1_t_best, img_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly the best brute force translation is not sensitive to the tolerance. Seems like a good thing. But translation doesn't work in this case. We could definitely optimise a homography to minimise this score though (non brute force)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tols = range(10,30,1)\n",
    "scores = [dd_score(img_1[10:,0:490], img_1_h[10:,0:490], img_2[10:,0:490], tol=tol) for tol in tols]\n",
    "plt.plot(tols, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair = img_pairs[12]\n",
    "\n",
    "pp_path = 'ds_bl'\n",
    "\n",
    "img_1 = cv.imread(os.path.join(base_path, 'data', pp_path, str(pair[0]) + '.png'), cv.IMREAD_GRAYSCALE)\n",
    "img_2 = cv.imread(os.path.join(base_path, 'data', pp_path, str(pair[1]) + '.png'), cv.IMREAD_GRAYSCALE)\n",
    "\n",
    "point_pairs = pd.read_csv(os.path.join(base_path, pp_path, str(pair[0]) + '_to_' + str(pair[1]) + '.csv'),\n",
    "                         index_col=0)\n",
    "\n",
    "before = point_pairs[['x_1','y_1']].values\n",
    "after = point_pairs[['x_2','y_2']].values\n",
    "\n",
    "h_matrix, mask = cv.findHomography(before, after, method=cv.RANSAC)\n",
    "img_1_h = cv.warpPerspective(img_1, h_matrix, dsize=img_1.shape)\n",
    "\n",
    "plt.figure(1, figsize=(17,9))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(f'Overlaid Images - {pair[0]} and {pair[1]}')\n",
    "plt.imshow(overlay(img_1, img_2))\n",
    "\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(f'Transformed Overlay - {pair[0]} and {pair[1]}')\n",
    "plt.imshow(overlay(img_1_h, img_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,16))\n",
    "\n",
    "for i in range(9):\n",
    "    plt.subplot(331 + i)\n",
    "    tol = i*5 + 5\n",
    "    show_diff_diff(img_1, img_1_h, img_2, crop=10, tol=tol)\n",
    "    plt.title(f'tol = {tol}')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tolerance range for this less contrasty image is noticably different from that of the highly contrast corrected images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_diff_hist(img_1, img_1_h, img_2, crop=10, tol=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "show_diff_diff(img_1, img_1_h, img_2, crop=10, tol=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are picking up mostly the right things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_score(img_1, img_1_h, img_2, crop=10, tol=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tols = range(10,30,1)\n",
    "scores = [dd_score(img_1, img_1_h, img_2, crop=10, tol=tol) for tol in tols]\n",
    "plt.plot(tols, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly where preprocessing has increased the contrast less, we will need to adjust the tolterances. But, this is only one parameter, and also we have shown that the tolerance will scale the score distribution, but should not affect it's shape too much, as long as we are in the right kind of ballpark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closer Look At Tolerance\n",
    "\n",
    "We have seen tolerance depends on the contrast in the images, so we should be able to detect the contrast and set the tolerance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair = img_pairs[12]\n",
    "\n",
    "pp_path = 'ds_bl'\n",
    "\n",
    "img_1_soft = cv.imread(os.path.join(base_path, 'data', pp_path, str(pair[0]) + '.png'), cv.IMREAD_GRAYSCALE)\n",
    "img_2_soft = cv.imread(os.path.join(base_path, 'data', pp_path, str(pair[1]) + '.png'), cv.IMREAD_GRAYSCALE)\n",
    "\n",
    "pp_path = 'clustered_norm'\n",
    "\n",
    "img_1_hard = cv.imread(os.path.join(base_path, 'data', pp_path, str(pair[0]) + '.png'), cv.IMREAD_GRAYSCALE)\n",
    "img_2_hard = cv.imread(os.path.join(base_path, 'data', pp_path, str(pair[1]) + '.png'), cv.IMREAD_GRAYSCALE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These images are obviously visually very different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(121)\n",
    "plt.imshow(img_1_soft, vmin=0, vmax=255)\n",
    "plt.subplot(122)\n",
    "plt.imshow(img_1_hard, vmin=0, vmax=255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img_1_soft.std(), img_1_hard.std())\n",
    "print(img_1_soft.mean(), img_1_hard.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img_2_soft.std(), img_2_hard.std())\n",
    "print(img_2_soft.mean(), img_2_hard.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However the metrics we have been using previously are completely unchanged! i.e. to our clustering etc algorithms there is no change between the images! So what is actually different about these two images? - _Local Contrast_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stds = []\n",
    "means = []\n",
    "meds = []\n",
    "\n",
    "for pair in img_pairs:\n",
    "    pp_path = 'ds_bl'\n",
    "\n",
    "    img_1_soft = cv.imread(os.path.join(base_path, 'data', pp_path, str(pair[0]) + '.png'), cv.IMREAD_GRAYSCALE)\n",
    "    img_2_soft = cv.imread(os.path.join(base_path, 'data', pp_path, str(pair[1]) + '.png'), cv.IMREAD_GRAYSCALE)\n",
    "\n",
    "    pp_path = 'clustered_norm'\n",
    "\n",
    "    img_1_hard = cv.imread(os.path.join(base_path, 'data', pp_path, str(pair[0]) + '.png'), cv.IMREAD_GRAYSCALE)\n",
    "    img_2_hard = cv.imread(os.path.join(base_path, 'data', pp_path, str(pair[1]) + '.png'), cv.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    std_1 = img_1_soft.std() - img_1_hard.std()\n",
    "    std_2 = img_2_soft.std() - img_2_hard.std()\n",
    "    mean_1 = img_1_soft.mean() - img_1_hard.mean()\n",
    "    mean_2 = img_2_soft.mean() - img_2_hard.mean()\n",
    "    med_1 = np.median(img_1_soft) - np.median(img_1_hard)\n",
    "    med_2 = np.median(img_2_soft) - np.median(img_2_hard)\n",
    "    \n",
    "    stds.append([std_1, std_2])\n",
    "    means.append([mean_1, mean_2])\n",
    "    meds.append([med_1, med_2])\n",
    "    \n",
    "stds = np.array(stds)\n",
    "means = np.array(means)\n",
    "meds = np.array(meds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stds.mean(), stds.std())\n",
    "print(means.mean(), means.std())\n",
    "print(meds.mean(), meds.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to look at local calculations of the above quantities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the window size\n",
    "def get_local_contrast(img, windowsize_r = 50, windowsize_c = 50):\n",
    "    stds = []\n",
    "    means = []\n",
    "\n",
    "    # crop out the window \n",
    "    for r in range(0,img.shape[0] - windowsize_r, windowsize_r):\n",
    "        for c in range(0,img.shape[1] - windowsize_c, windowsize_c):\n",
    "            window = img[r:r+windowsize_r,c:c+windowsize_c]\n",
    "            means.append(window.mean())\n",
    "            stds.append(window.std())\n",
    "\n",
    "    mean = np.array(means).mean(), np.array(means).std()\n",
    "    std = np.array(stds).mean(), np.array(stds).std()\n",
    "    \n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std = get_local_contrast(img_1_hard)\n",
    "print(mean)\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std = get_local_contrast(img_1_soft)\n",
    "print(mean)\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_1 = []\n",
    "pp_2 = []\n",
    "\n",
    "for id_ in availible_ids:\n",
    "    pp_path = 'ds_bl'\n",
    "\n",
    "    img_1 = cv.imread(os.path.join(base_path, 'data', pp_path, str(id_) + '.png'), cv.IMREAD_GRAYSCALE)\n",
    "\n",
    "    pp_path = 'clustered_norm'\n",
    "\n",
    "    img_2 = cv.imread(os.path.join(base_path, 'data', pp_path, str(id_) + '.png'), cv.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    mean_1, std_1 = get_local_contrast(img_1)\n",
    "    mean_2, std_2 = get_local_contrast(img_2)\n",
    "    \n",
    "    pp_1.append([mean_1, std_1])\n",
    "    pp_2.append([mean_2, std_2])\n",
    "    \n",
    "pp_1 = np.array(pp_1)\n",
    "pp_2 = np.array(pp_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean of each subblock\n",
    "mean_mean_mean = np.round(pp_1[:,0,0].mean(), 2)\n",
    "mean_std_mean = np.round(pp_1[:,0,1].mean(), 2)\n",
    "std_mean_mean = np.round(pp_1[:,0,0].std(), 2)\n",
    "std_std_mean = np.round(pp_1[:,0,1].std(), 2)\n",
    "\n",
    "# variance of each subblock\n",
    "mean_mean_std = np.round(pp_1[:,1,0].mean(), 2)\n",
    "mean_std_std = np.round(pp_1[:,1,1].mean(), 2)\n",
    "std_mean_std = np.round(pp_1[:,1,0].std(), 2)\n",
    "std_std_std = np.round(pp_1[:,1,1].std(), 2)\n",
    "\n",
    "print('LOW CLAHE PP')\n",
    "print('Distribution (over images) of mean (over subblocks) of mean (over intinsities)')\n",
    "print(mean_mean_mean, '+\\-', std_mean_mean)\n",
    "print('(average pixel intensity accross all images)')\n",
    "\n",
    "print('\\nDistribution (over images) of mean (over subblocks) of std (over intinsities)')\n",
    "print(mean_mean_std, '+\\-', std_mean_std)\n",
    "print('(on average, subblocks have less contrast)')\n",
    "\n",
    "print('\\nDistribution (over images) of std (over subblocks) of mean (over intinsities)')\n",
    "print(mean_std_mean, '+\\-', std_std_mean)\n",
    "print('(average subblock intensity is more variable)')\n",
    "\n",
    "print('\\nDistribution (over images) of std (over subblocks) of std (over intinsities)')\n",
    "print(mean_std_std, '+\\-', std_std_std)\n",
    "print('??')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean of each subblock\n",
    "mean_mean_mean = np.round(pp_2[:,0,0].mean(), 2)\n",
    "mean_std_mean = np.round(pp_2[:,0,1].mean(), 2)\n",
    "std_mean_mean = np.round(pp_2[:,0,0].std(), 2)\n",
    "std_std_mean = np.round(pp_2[:,0,1].std(), 2)\n",
    "\n",
    "# variance of each subblock\n",
    "mean_mean_std = np.round(pp_2[:,1,0].mean(), 2)\n",
    "mean_std_std = np.round(pp_2[:,1,1].mean(), 2)\n",
    "std_mean_std = np.round(pp_2[:,1,0].std(), 2)\n",
    "std_std_std = np.round(pp_2[:,1,1].std(), 2)\n",
    "\n",
    "print('HIGH CLAHE PP')\n",
    "print('Distribution (over images) of mean (over subblocks) of mean (over intinsities)')\n",
    "print(mean_mean_mean, '+\\-', std_mean_mean)\n",
    "print('(average pixel intensity accross all images)')\n",
    "\n",
    "print('\\nDistribution (over images) of mean (over subblocks) of std (over intinsities)')\n",
    "print(mean_mean_std, '+\\-', std_mean_std)\n",
    "print('(on average, subblocks have more contrast)')\n",
    "\n",
    "print('\\nDistribution (over images) of std (over subblocks) of mean (over intinsities)')\n",
    "print(mean_std_mean, '+\\-', std_std_mean)\n",
    "print('(average subblock intensity is less variable)')\n",
    "\n",
    "print('\\nDistribution (over images) of std (over subblocks) of std (over intinsities)')\n",
    "print(mean_std_std, '+\\-', std_std_std)\n",
    "print('??')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a new metric in some of these we can use. We can probably automatically set the tolerance based on how much CLAHE has been performed, which will be easy to find out using these new metrics.\n",
    "\n",
    "ALSO you could calculate the contrast properties of each image, and then have a asymmetric tolerance for changes for each such that you can match images whose contrasts are not the similar. This could then be done at the subblock level, which hopefully would mitigate the fail cases described above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of preprocessing, from the way that this new score is calculated (i.e. reliant on per pixel changes) it makes sense that noise in the image would be our biggest enemy, especially if we can get some resilience to different contrasts through "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto Tolerance\n",
    "\n",
    "Initial assumption that all images will have similar amounts of pp, i.e. we are determining the tolerance for the entire imgbank (to remove differences between very light and very harsh pp). We could make this per image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the window size\n",
    "def get_local_contrast(img, windowsize_r = 50, windowsize_c = 50):\n",
    "    stds = []\n",
    "    means = []\n",
    "\n",
    "    # crop out the window \n",
    "    for r in range(0,img.shape[0] - windowsize_r, windowsize_r):\n",
    "        for c in range(0,img.shape[1] - windowsize_c, windowsize_c):\n",
    "            window = img[r:r+windowsize_r,c:c+windowsize_c]\n",
    "            means.append(window.mean())\n",
    "            stds.append(window.std())\n",
    "\n",
    "    mean = np.array(means).mean(), np.array(means).std()\n",
    "    std = np.array(stds).mean(), np.array(stds).std()\n",
    "    \n",
    "    mean_subblock_contrast = std[0]\n",
    "    variance_between_subblocks = mean[1]\n",
    "    std_std = std[1]\n",
    "    \n",
    "    return mean_subblock_contrast, variance_between_subblocks, std_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pair = img_pairs[12]\n",
    "\n",
    "pp_path = 'clustered_norm'\n",
    "\n",
    "img_1 = cv.imread(os.path.join(base_path, 'data', pp_path, str(pair[0]) + '.png'), cv.IMREAD_GRAYSCALE)\n",
    "img_2 = cv.imread(os.path.join(base_path, 'data', pp_path, str(pair[1]) + '.png'), cv.IMREAD_GRAYSCALE)\n",
    "\n",
    "point_pairs = pd.read_csv(os.path.join(base_path, pp_path, str(pair[0]) + '_to_' + str(pair[1]) + '.csv'),\n",
    "                         index_col=0)\n",
    "\n",
    "before = point_pairs[['x_1','y_1']].values\n",
    "after = point_pairs[['x_2','y_2']].values\n",
    "\n",
    "h_matrix, mask = cv.findHomography(before, after, method=cv.RANSAC)\n",
    "img_1_h = cv.warpPerspective(img_1, h_matrix, dsize=img_1.shape)\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(1, figsize=(17,9))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(f'Overlaid Images - {pair[0]} and {pair[1]}')\n",
    "plt.imshow(overlay(img_1, img_2)[300:500,300:500], vmin=0, vmax=255)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(f'Transformed Overlay - {pair[0]} and {pair[1]}')\n",
    "plt.imshow(overlay(img_1_h, img_2)[300:500,300:500], vmin=0, vmax=255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stratergy will be to try several images from the same pp bundle and measure tolerance and local contrast for each. Then compare between different bundles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_1[300:500,300:500].mean() - img_2[300:500,300:500].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,16))\n",
    "\n",
    "tols = np.linspace(10,50,9).astype(int)\n",
    "\n",
    "for i in range(9):\n",
    "    plt.subplot(331 + i)\n",
    "    show_diff_diff(img_1, img_1_h, img_2, crop=10, tol=tols[i])\n",
    "    plt.title(f'tol = {tols[i]}')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_score(img_1, img_1_h, img_2, crop=10, tol=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(np.array([get_local_contrast(img_1),\n",
    "          get_local_contrast(img_1_h),\n",
    "          get_local_contrast(img_2)]).mean(axis=0), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After looking through several cases, I numerically fit the tolerance to using a linear function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tol(img):\n",
    "    mean_subblock_contrast = get_local_contrast(img)[0]\n",
    "    return mean_subblock_contrast*1.12 + 1.88"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score Normalisation\n",
    "\n",
    "We don't want our score to be dependend on the contrast in the images. Different image contrasts necessisstate different tolerances, and the differing contrast and the change in tolerance can both change the absolute value of the score. What we can do is take a matched trio of images and count the score for a range of different contrast on this match. Then we can deduce how the scores depend on the contrast in the images, and correct for that dependence.\n",
    "\n",
    "Start out by doing the same CC to all three images, but could probably generalise to have different CC to each image in the match set.\n",
    "\n",
    "[this is unfinished, instead we fixed scoring preprocessing independently of matching preprocessing]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_contrast(img, clip_limit, n_tiles_per_row):\n",
    "    \n",
    "    # note that the actual affects of the clip limit \n",
    "    # depend on the normalisation of the histogram\n",
    "    # and therefore the number of tiles in the grid\n",
    "    \n",
    "    # create the object\n",
    "    clahe = cv.createCLAHE(clipLimit=clip_limit, \n",
    "                           tileGridSize=(n_tiles_per_row, n_tiles_per_row))\n",
    "    \n",
    "    # apply equalisation\n",
    "    return clahe.apply(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair = img_pairs[12]\n",
    "\n",
    "pp_path = 'ds_bl_nlm'\n",
    "\n",
    "img_1 = cv.imread(os.path.join(base_path, 'data', pp_path, str(pair[0]) + '.png'), cv.IMREAD_GRAYSCALE)\n",
    "img_2 = cv.imread(os.path.join(base_path, 'data', pp_path, str(pair[1]) + '.png'), cv.IMREAD_GRAYSCALE)\n",
    "\n",
    "point_pairs = pd.read_csv(os.path.join(base_path, pp_path, str(pair[0]) + '_to_' + str(pair[1]) + '.csv'),\n",
    "                         index_col=0)\n",
    "\n",
    "before = point_pairs[['x_1','y_1']].values\n",
    "after = point_pairs[['x_2','y_2']].values\n",
    "\n",
    "h_matrix, mask = cv.findHomography(before, after, method=cv.RANSAC)\n",
    "img_1_h = cv.warpPerspective(img_1, h_matrix, dsize=img_1.shape)\n",
    "\n",
    "plt.figure(1, figsize=(10,9))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(f'Overlaid Images - {pair[0]} and {pair[1]}')\n",
    "plt.imshow(overlay(img_1, img_2))\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(f'Transformed Overlay - {pair[0]} and {pair[1]}')\n",
    "plt.imshow(overlay(img_1_h, img_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frist let's manually find the app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,16))\n",
    "\n",
    "for i in range(9):\n",
    "    plt.subplot(331 + i)\n",
    "    show_diff_diff(img_1, img_1_h, img_2, crop=5, tol=i*10+10)\n",
    "    plt.title(f'tol = {i*10 + 10}')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clips = np.linspace(0.00000001, 10, 20)\n",
    "\n",
    "img_1s = [correct_contrast(img_1, clip, 20) for clip in clips]\n",
    "img_1_hs = [correct_contrast(img_1_h, clip, 20) for clip in clips]\n",
    "img_2s = [correct_contrast(img_2, clip, 20) for clip in clips]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tols = [np.array([get_tol(img_1), get_tol(img_1_h), get_tol(img_2)]).mean() \n",
    "        for img_1, img_1_h, img_2 in zip(img_1s, img_1_hs, img_2s)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[dd_score(img_1, img_1_h, img_2, tol=tol) for img_1, img_1_h, img_2, tol in zip(img_1s, img_1_hs, img_2s, tols)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise removal\n",
    "\n",
    "Notive a lot of salt and pepper noise on the score from dirt in the optics and other sources of noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair = img_pairs[12]\n",
    "\n",
    "pp_path = 'ds_bl_nlm'\n",
    "\n",
    "img_1 = cv.imread(os.path.join(base_path, 'data', pp_path, str(pair[0]) + '.png'), cv.IMREAD_GRAYSCALE)\n",
    "img_2 = cv.imread(os.path.join(base_path, 'data', pp_path, str(pair[1]) + '.png'), cv.IMREAD_GRAYSCALE)\n",
    "\n",
    "point_pairs = pd.read_csv(os.path.join(base_path, pp_path, str(pair[0]) + '_to_' + str(pair[1]) + '.csv'),\n",
    "                         index_col=0)\n",
    "\n",
    "before = point_pairs[['x_1','y_1']].values\n",
    "after = point_pairs[['x_2','y_2']].values\n",
    "\n",
    "h_matrix, mask = cv.findHomography(before, after, method=cv.RANSAC)\n",
    "img_1_h = cv.warpPerspective(img_1, h_matrix, dsize=img_1.shape)\n",
    "\n",
    "show_diff_diff(img_1, img_1_h, img_2, tol=9, crop=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reduce this by applying a median filter to the scoring layer of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol = 7\n",
    "\n",
    "img_1 = crop_image(img_1)\n",
    "img_1_h = crop_image(img_1_h)\n",
    "img_2 = crop_image(img_2)\n",
    "\n",
    "diff_before = np.abs(img_1.astype(int) - img_2.astype(int))\n",
    "diff_after = np.abs(img_1_h.astype(int) - img_2.astype(int))\n",
    "diff_diff = diff_before - diff_after\n",
    "\n",
    "better = np.zeros_like(diff_diff)\n",
    "worse = np.zeros_like(diff_diff)\n",
    "\n",
    "better[diff_diff > tol] = 1\n",
    "worse[diff_diff < -tol] = 1\n",
    "\n",
    "better_med_3 = cv.medianBlur(better.astype('uint8'),3)\n",
    "worse_med_3 = cv.medianBlur(worse.astype('uint8'),3)\n",
    "\n",
    "better_med_5 = cv.medianBlur(better.astype('uint8'),5)\n",
    "worse_med_5 = cv.medianBlur(worse.astype('uint8'),5)\n",
    "\n",
    "plt.figure(figsize=(17,10))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.gca().get_xaxis().set_visible(False)\n",
    "plt.gca().get_yaxis().set_visible(False)\n",
    "plt.title('raw score')\n",
    "img_1_c = img_1.copy()\n",
    "img_1_c[better == 1] = 255\n",
    "img_1_c[worse == 1] = 0\n",
    "plt.imshow(img_1_c)\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.gca().get_xaxis().set_visible(False)\n",
    "plt.gca().get_yaxis().set_visible(False)\n",
    "plt.title('median blur r=3')\n",
    "img_1_c = img_1.copy()\n",
    "img_1_c[better_med_3 == 1] = 255\n",
    "img_1_c[worse_med_3 == 1] = 0\n",
    "plt.imshow(img_1_c)\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.gca().get_xaxis().set_visible(False)\n",
    "plt.gca().get_yaxis().set_visible(False)\n",
    "plt.title('median blur r=5')\n",
    "img_1_c = img_1.copy()\n",
    "img_1_c[better_med_5 == 1] = 255\n",
    "img_1_c[worse_med_5 == 1] = 0\n",
    "plt.tight_layout()\n",
    "plt.imshow(img_1_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does come at the cost of some reduced signal, but in the `r=3` case signal lossess are minimal and noise is dramatically supressed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score with Subblocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddscore_sub(img_1, img_1_h, img_2, tol, crop):\n",
    "    \"\"\"\n",
    "    understand better the behaviour of subtracting pixel\n",
    "    intensities: what happens of we go outside [0,255]?\n",
    "    (at the moment we are just converting to int and then\n",
    "    forgetting about it)\n",
    "    \"\"\"\n",
    "\n",
    "    #crop = 10\n",
    "    newlen = img_1.shape[0] - 2*crop\n",
    "\n",
    "    windowsize_r = 96\n",
    "    windowsize_c = 96   \n",
    "    \n",
    "    windowsize_r = newlen // 5\n",
    "    windowsize_c = newlen // 5   \n",
    "    \n",
    "    img_1_COPY = img_1.copy()\n",
    "    img_1_h_COPY = img_1_h.copy()\n",
    "    img_2_COPY = img_2.copy()\n",
    "    \n",
    "    img_1 = crop_image(img_1, b=crop).astype(int)[crop:img_1.shape[0]-crop,crop:img_1.shape[0]-crop]\n",
    "    img_1_h = crop_image(img_1_h, b=crop).astype(int)[crop:img_1_h.shape[0]-crop,crop:img_1_h.shape[0]-crop]\n",
    "    img_2 = crop_image(img_2, b=crop).astype(int)[crop:img_2.shape[0]-crop,crop:img_2.shape[0]-crop]\n",
    "    scores = []\n",
    "    counts = []\n",
    "\n",
    "    # crop out the window \n",
    "    for r in range(0,img_1.shape[0] - windowsize_r + 1, windowsize_r):\n",
    "        for c in range(0,img_1.shape[1] - windowsize_c + 1, windowsize_c):\n",
    "            #print(r,c)\n",
    "            img_1_sb = img_1[r:r+windowsize_r,c:c+windowsize_c]\n",
    "            img_1_h_sb = img_1_h[r:r+windowsize_r,c:c+windowsize_c]\n",
    "            img_2_sb = img_2[r:r+windowsize_r,c:c+windowsize_c]\n",
    "            \n",
    "            mean_diff = img_2_sb.mean() - img_1_sb.mean()\n",
    "            img_2_sb -= int(np.round(mean_diff))\n",
    "            img_2_sb = np.clip(img_2_sb, 0, 255)\n",
    "            \n",
    "            score, count = dd_score(img_1_sb, img_1_h_sb, img_2_sb, tol=tol, crop=0)\n",
    "            scores.append(score)\n",
    "            counts.append(count)\n",
    "    \n",
    "    scores = np.array(scores)\n",
    "    counts = np.array(counts)\n",
    "    debug=False\n",
    "    if debug:\n",
    "        print(\"I HAVE A NEW IMAGE\")\n",
    "        print(scores)\n",
    "        print(counts)\n",
    "        show_dd_sb(img_1_COPY, img_1_h_COPY, img_2_COPY, tol=tol, crop=crop)\n",
    "    \n",
    "\n",
    "    if counts.sum() < 100:\n",
    "        #print('none')\n",
    "        return 0.5,0\n",
    "    \n",
    "    scores[counts < 20] = 0.5\n",
    "    counts[counts < 20] = 0\n",
    "    #scores = scores[counts > 50]\n",
    "    #print(scores)\n",
    "    #if len(scores) == 0:\n",
    "    #   print('none')\n",
    "    #    return 0.5,0\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    if counts.sum() == 0:\n",
    "        #print('none')\n",
    "        return 0.5,0\n",
    "    if debug:\n",
    "        print(scores)\n",
    "        print(counts)\n",
    "        print(np.average(scores,weights=counts))\n",
    "    return np.average(scores,weights=counts), counts.sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair = img_pairs[12]\n",
    "\n",
    "pp_path = 'clustered_norm'\n",
    "\n",
    "img_1 = cv.imread(os.path.join(base_path, 'data', pp_path, str(pair[0]) + '.png'), cv.IMREAD_GRAYSCALE)\n",
    "img_2 = cv.imread(os.path.join(base_path, 'data', pp_path, str(pair[1]) + '.png'), cv.IMREAD_GRAYSCALE)\n",
    "\n",
    "\n",
    "point_pairs = pd.read_csv(os.path.join(base_path, pp_path, str(pair[0]) + '_to_' + str(pair[1]) + '.csv'),\n",
    "                         index_col=0)\n",
    "\n",
    "before = point_pairs[['x_1','y_1']].values\n",
    "after = point_pairs[['x_2','y_2']].values\n",
    "\n",
    "h_matrix, mask = cv.findHomography(before, after, method=cv.RANSAC)\n",
    "img_1_h = cv.warpPerspective(img_1, h_matrix, dsize=img_1.shape)\n",
    "\n",
    "\n",
    "plt.figure(1, figsize=(17,4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(f'Overlaid Images - {pair[0]} and {pair[1]}')\n",
    "plt.imshow(overlay(img_1, img_2)[300:500,300:500], vmin=0, vmax=255)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(f'Transformed Overlay - {pair[0]} and {pair[1]}')\n",
    "plt.imshow(overlay(img_1_h, img_2)[300:500,300:500], vmin=0, vmax=255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_dd_sb(img_1, img_1_h, img_2, tol, crop):\n",
    "    \"\"\"\n",
    "    Visualisation for the subblock scoring.\n",
    "    Much better implementations are probably possible.\n",
    "    \"\"\"\n",
    "    \n",
    "    newlen = img_1.shape[0] - 2*crop\n",
    "    \n",
    "    windowsize_r = newlen // 5\n",
    "    windowsize_c = newlen // 5   \n",
    "    \n",
    "    img_1 = crop_image(img_1, b=crop).astype(int)[crop:img_1.shape[0]-crop,crop:img_1.shape[0]-crop]\n",
    "    img_1_h = crop_image(img_1_h, b=crop).astype(int)[crop:img_1_h.shape[0]-crop,crop:img_1_h.shape[0]-crop]\n",
    "    img_2 = crop_image(img_2, b=crop).astype(int)[crop:img_2.shape[0]-crop,crop:img_2.shape[0]-crop]\n",
    "    \n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.subplot(121)\n",
    "    plt.imshow(overlay(img_1, img_2), vmin=0, vmax=255)\n",
    "    plt.subplot(122)\n",
    "    plt.imshow(overlay(img_1_h, img_2), vmin=0, vmax=255)\n",
    "    plt.show()\n",
    "\n",
    "    figa, axs = plt.subplots(5,5, figsize=(7,7), sharey=True, sharex=True)\n",
    "    figa.subplots_adjust(wspace=-0.1, hspace=0)\n",
    "\n",
    "    for i, r in enumerate(range(0,img_1.shape[0] - windowsize_r+1, windowsize_r)):\n",
    "        for j, c in enumerate(range(0,img_1.shape[1] - windowsize_c+1, windowsize_c)):\n",
    "\n",
    "            img_1_sb = img_1[r:r+windowsize_r,c:c+windowsize_c]\n",
    "            img_1_h_sb = img_1_h[r:r+windowsize_r,c:c+windowsize_c]\n",
    "            img_2_sb = img_2[r:r+windowsize_r,c:c+windowsize_c]\n",
    "\n",
    "            mean_diff = img_2_sb.mean() - img_1_sb.mean()\n",
    "            img_2_sb -= int(np.round(mean_diff))\n",
    "            img_2_sb = np.clip(img_2_sb, 0, 255)\n",
    "            \n",
    "            dd = get_diff_diff(img_1_sb, img_1_h_sb, img_2_sb, crop=0, tol=tol, flat_filtered=False)\n",
    "            overl = overlay(img_1_sb, img_2_sb)\n",
    "            overl[np.where(dd > tol)] = 255\n",
    "            overl[np.where(dd < -tol)] = 0\n",
    "            axs[i,j].imshow(overl, vmin=0, vmax=255)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "show_dd_sb(img_1.astype(int), img_1_h.astype(int), img_2.astype(int), tol=30, crop=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring PP with DDScore\n",
    "\n",
    "<img src=\"https://i.imgur.com/vdi6TKt.png\" alt=\"drawing\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# methods we have run deepmatching for that are to be scored\n",
    "pp_methods = ['unpp', 'ds', 'clustered_norm', 'ds_cc_ds',\n",
    "              'ds_cc_bl', 'ds_bl', 'ds_cc_bl_lite', 'ds_cc_bl_nlm',\n",
    "              'ds_bl_nlm', 'ds_cc_bl_nlm_heavy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_pp(pp_method, score_path, tol):\n",
    "    \"\"\"\n",
    "    Globally score all images preprocessed in the same way\n",
    "    that are found in the path specified by pp_method. Using\n",
    "    scoring preprocessing from score_path.\n",
    "    \"\"\"\n",
    "\n",
    "    pp_img_path = os.path.join(base_path, 'data', pp_method)\n",
    "    hom_path = os.path.join(base_path, pp_method)\n",
    "    \n",
    "    scores = []\n",
    "    counts = []\n",
    "\n",
    "    for img_pair in img_pairs:\n",
    "\n",
    "        img_1_id, img_2_id = img_pair\n",
    "\n",
    "        img_1 = cv.imread(os.path.join(score_path, str(img_1_id) + '.png'), cv.IMREAD_GRAYSCALE)\n",
    "        img_2 = cv.imread(os.path.join(score_path, str(img_2_id) + '.png'), cv.IMREAD_GRAYSCALE)\n",
    "\n",
    "        point_pairs = pd.read_csv(os.path.join(hom_path, str(img_1_id) + '_to_' + str(img_2_id) + '.csv'),\n",
    "                                 index_col=0)\n",
    "\n",
    "        before = point_pairs[['x_1','y_1']].values\n",
    "        after = point_pairs[['x_2','y_2']].values\n",
    "\n",
    "        h_matrix, mask = cv.findHomography(before, after, method=cv.RANSAC)\n",
    "        img_1_h = cv.warpPerspective(img_1, h_matrix, dsize=img_1.shape)\n",
    "\n",
    "        score, count = dd_score(img_1, img_1_h, img_2, tol=tol, crop=7)\n",
    "\n",
    "        scores.append(score)\n",
    "        counts.append(count)\n",
    "\n",
    "\n",
    "    scores = np.array(scores)\n",
    "    counts = np.array(counts)\n",
    "    \n",
    "    mean_count = counts.mean()\n",
    "    \n",
    "    frac_unscored = len(counts[counts == 0]) / len(scores)\n",
    "\n",
    "    changed_score_mean = scores[scores != -1].mean()\n",
    "    changed_score_std = scores[scores != -1].std()\n",
    "\n",
    "    scores[scores == -1] = 0.5\n",
    "    scores[counts < 50] = 0.5\n",
    "    score_mean = scores.mean()\n",
    "    score_std = scores.std()\n",
    "    \n",
    "    return  [mean_count, frac_unscored, changed_score_mean, \n",
    "             changed_score_std,score_mean, \n",
    "             score_std]\n",
    "\n",
    "\n",
    "\n",
    "def score_pp_sb(pp_method, score_path, tol, crop):\n",
    "    \"\"\"\n",
    "    Score images by breaking them up into subblocks\n",
    "    \"\"\"\n",
    "    print(pp_method)\n",
    "    pp_img_path = os.path.join(base_path, 'data', pp_method)\n",
    "    hom_path = os.path.join(base_path, pp_method)\n",
    "    \n",
    "    best_pair = 0\n",
    "    best_score = 0\n",
    "    best_index = -1\n",
    "    worst_pair = 0\n",
    "    worst_score = 1\n",
    "    worst_index=-1\n",
    "    \n",
    "    scores = []\n",
    "    counts = []\n",
    "\n",
    "    for i, img_pair in enumerate(img_pairs):\n",
    "        #print(\"IMAGE PAIR\")\n",
    "        #print(img_pair)\n",
    "        img_1_id, img_2_id = img_pair\n",
    "\n",
    "        img_1 = cv.imread(os.path.join(score_path, str(img_1_id) + '.png'), cv.IMREAD_GRAYSCALE)\n",
    "        img_2 = cv.imread(os.path.join(score_path, str(img_2_id) + '.png'), cv.IMREAD_GRAYSCALE)\n",
    "\n",
    "        point_pairs = pd.read_csv(os.path.join(hom_path, str(img_1_id) + '_to_' + str(img_2_id) + '.csv'),\n",
    "                                 index_col=0)\n",
    "\n",
    "        before = point_pairs[['x_1','y_1']].values\n",
    "        after = point_pairs[['x_2','y_2']].values\n",
    "\n",
    "        h_matrix, mask = cv.findHomography(before, after, method=cv.RANSAC)\n",
    "        img_1_h = cv.warpPerspective(img_1, h_matrix, dsize=img_1.shape)\n",
    "\n",
    "        score, count = ddscore_sub(img_1, img_1_h, img_2, tol=tol, crop=crop)\n",
    "        \n",
    "        #if score > 0.5:\n",
    "            #print(i)\n",
    "            #print(score)\n",
    "        if score > best_score and count > 1:\n",
    "            #print(i)\n",
    "            best_score = score\n",
    "            best_pair = img_pair\n",
    "            best_index = i\n",
    "        if score < worst_score and count > 1:\n",
    "            worst_score = score\n",
    "            worst_pair = img_pair\n",
    "            worst_index =i\n",
    "\n",
    "        scores.append(score)\n",
    "        counts.append(count)\n",
    "\n",
    "    print('best')\n",
    "    print(best_score)\n",
    "    print(best_index, best_pair)\n",
    "    \n",
    "    print('worst')\n",
    "    print(worst_score)\n",
    "    print(worst_index, worst_pair)\n",
    "    \n",
    "    scores = np.array(scores)\n",
    "    counts = np.array(counts)\n",
    "    \n",
    "    mean_count = counts.mean()\n",
    "    \n",
    "    frac_unscored = len(counts[counts == 0]) / len(scores)\n",
    "\n",
    "    changed_score_mean = scores[scores != 0.5].mean()\n",
    "    changed_score_std = scores[scores != 0.5].std()\n",
    "\n",
    "    scores[scores == 0.5] = 0.5\n",
    "    score_mean = scores.mean()\n",
    "    score_std = scores.std()\n",
    "    \n",
    "    num_better = len(scores[scores > 0.5])\n",
    "    num_worse = len(scores[scores < 0.5])\n",
    "    num_unscored = len(scores[scores == 0.5])\n",
    "    mean_better = scores[scores > 0.5].mean()\n",
    "    mean_worse = scores[scores < 0.5].mean()\n",
    "    std_better = scores[scores > 0.5].std()\n",
    "    std_worse = scores[scores < 0.5].std()\n",
    "    \n",
    "    return  [mean_count, frac_unscored, changed_score_mean, \n",
    "             changed_score_std,score_mean, score_std,\n",
    "             num_better, num_worse, num_unscored, \n",
    "             mean_better, std_better,\n",
    "             mean_worse, std_worse]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## subblock scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "thoughts:\n",
    "- don't fully understand behaviour above, more detailed look needed (but no time). Why scores less than 0.5? (when some are above 0.5)\n",
    "- still looks like dependence of score on base images\n",
    "- maybe can improve stds with more pairs\n",
    "- what to do with unscored images (these squash the means, making it harder to differentiate different pp)\n",
    "- need to rereun clustering and pipeline with new metrics\n",
    "- Possible final result for report: more pairs, comparison of unpp/clustered/pipeline - show improvement from unpp, more work needed to differentiate between differet pp, both are simiarly good\n",
    "- separate out images that have significantly moved!!!\n",
    "- normalise/regularise score with simulation\n",
    "\n",
    "plan:\n",
    "1. redo top pp methods\n",
    "2. more image pairs\n",
    "3. separate out images iwth significant changes\n",
    "4. show fixed examples and failc cases - try and explain which are fixed, why that many are fixed (i.e. many pairs do not change significantly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing Interesting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pair = img_pairs[174]\n",
    "#pair = img_pairs[81]\n",
    "#pair = img_pairs[121]\n",
    "#pair = img_pairs[244]\n",
    "#pair = img_pairs[292]\n",
    "#pair = img_pairs[121]\n",
    "\n",
    "# unpp test best\n",
    "pair = img_pairs[42]\n",
    "pair = img_pairs[46]\n",
    "pair = img_pairs[6]\n",
    "\n",
    "# worst unpp\n",
    "#pair = img_pairs[95]\n",
    "\n",
    "#pp_path = 'final-superheavy'\n",
    "pp_path = 'final-unpp'\n",
    "img_1 = cv.imread(os.path.join(base_path, 'data', 'clustered_norm', str(pair[0]) + '.png'), cv.IMREAD_GRAYSCALE)\n",
    "img_2 = cv.imread(os.path.join(base_path, 'data', 'clustered_norm', str(pair[1]) + '.png'), cv.IMREAD_GRAYSCALE)\n",
    "\n",
    "\n",
    "point_pairs = pd.read_csv(os.path.join(base_path, pp_path, str(pair[0]) + '_to_' + str(pair[1]) + '.csv'),\n",
    "                         index_col=0)\n",
    "\n",
    "before = point_pairs[['x_1','y_1']].values\n",
    "after = point_pairs[['x_2','y_2']].values\n",
    "\n",
    "h_matrix, mask = cv.findHomography(before, after, method=cv.RANSAC)\n",
    "img_1_h = cv.warpPerspective(img_1, h_matrix, dsize=img_1.shape)\n",
    "\n",
    "\n",
    "'''plt.figure(1, figsize=(12,8))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.gca().get_xaxis().set_visible(False)\n",
    "plt.gca().get_yaxis().set_visible(False)\n",
    "plt.title(f'Overlay - {pair[0]} and {pair[1]}')\n",
    "plt.imshow(overlay(img_1, img_2), vmin=0, vmax=255)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.gca().get_xaxis().set_visible(False)\n",
    "plt.gca().get_yaxis().set_visible(False)\n",
    "plt.title(f'Transformed Overlay - {pair[0]} and {pair[1]}')\n",
    "plt.imshow(overlay(img_1_h, img_2), vmin=0, vmax=255)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_dd_sb(img_1.astype(int), img_1_h.astype(int), img_2.astype(int), tol=50, crop=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- what to do aboutserious problem with cropping leading to bad scores? just crop everything alot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pp_methods = ['unpp', 'final-ds', 'final-light', 'final-heavy', 'final-superheavy']\n",
    "\n",
    "score_base = 'final-superheavy'\n",
    "score_path = os.path.join(base_path, 'data', score_base)\n",
    "\n",
    "ddscore_df = pd.DataFrame(columns=['mean_count', 'frac_unscored', 'changed_score_mean', \n",
    "                                   'changed_score_std', 'score_mean', 'score_std',\n",
    "                                   'num_better', 'num_worse', 'num_unscored',\n",
    "                                   'mean_better', 'std_better',\n",
    "                                   'mean_worse', 'std_worse'])\n",
    "\n",
    "for pp_method in pp_methods:\n",
    "    ddscore_df.loc[pp_method] = score_pp_sb(pp_method, score_path, tol=50, crop=20)\n",
    "\n",
    "print(score_base)\n",
    "ddscore_df.sort_values(by='changed_score_mean', ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learnings:\n",
    "- Previously assumed a low `frac_unscored` was good, but actually a substantial fraction of randomly generated image pairs don't need to be corrected much, so it makes sense that this does not go to zero\n",
    "- As such, tolerances have been increased, and we basically reject less noise. Before we were just seeing noise\n",
    "- ?stds so big because substantial fraction of image pairs get worse, and so are < 0.5 and spread the distribution right out\n",
    "- separating out improved and worsened pairs we have substantiated the above.\n",
    "- perhaps the most useful metric is the raw number of images that we improve the match of. i.e. you can always reject matches that haven't worked and not use them. in this case the final-superheavy pp is the best as we have the most improved images.\n",
    "- doing this with images where we 100% should see changes (selecting large changes) will be interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# massage the DataFrame so we can copy it straight to LaTeX\n",
    "\n",
    "dddfinal = ddscore_df.drop(columns=['score_mean', 'score_std', 'frac_unscored', 'mean_count']).sort_values(by='num_better', ascending=False)\n",
    "dddfinal = dddfinal.sort_values(by='num_better', ascending=True).drop(columns=['changed_score_mean', 'changed_score_std'])\n",
    "dddfinal[['num_better', 'num_worse', 'num_unscored']] = dddfinal[['num_better', 'num_worse', 'num_unscored']].astype(int)\n",
    "dddfinal[['mean_better', 'std_better', 'mean_worse', 'std_worse']] = np.round(dddfinal[['mean_better', 'std_better', 'mean_worse', 'std_worse']],2)\n",
    "\n",
    "dddfinal = dddfinal.iloc[[0,1,2,3,4]]\n",
    "\n",
    "dddfinal.index = ['Unpreprocessed', 'Downsampled', 'Sequential Light', 'Sequential Heavy', 'Sequential Superheavy']\n",
    "dddfinal = dddfinal.rename({'num_better': '$n_s$',\n",
    "                            'num_worse': '$n_f$',\n",
    "                            'num_unscored': '$n_u$',\n",
    "                            'mean_better': 'success score',\n",
    "                            'mean_worse': 'fail score'}, axis='columns')\n",
    "\n",
    "\n",
    "dddfinal['success score'] = dddfinal['success score'].astype('str')\n",
    "dddfinal['std_better'] = dddfinal['std_better'].astype('str')\n",
    "dddfinal['fail score'] = dddfinal['fail score'].astype('str')\n",
    "dddfinal['std_worse'] = dddfinal['std_worse'].astype('str')\n",
    "\n",
    "\n",
    "\n",
    "dddfinal['success score'] = '$' + dddfinal['success score'] + ' \\pm ' + dddfinal['std_better'] + '$'\n",
    "dddfinal['fail score'] = '$' + dddfinal['fail score'] + ' \\pm ' + dddfinal['std_worse'] + '$'\n",
    "\n",
    "dddfinal = dddfinal.drop(columns=['std_better', 'std_worse'])\n",
    "dddfinal = dddfinal[['$n_s$', 'success score', '$n_f$', 'fail score', '$n_u$']]\n",
    "dddfinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = dddfinal.to_latex('/Users/sam/Desktop/data.txt', bold_rows=True, column_format='lrrrrrr', escape=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do things like look for a good match in any of the candidate pp strategies (i.e. we do not restrict ourselves to one pp strategy but search for a good match for image pair X across several strategies)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Unpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_methods = ['unpp']\n",
    "\n",
    "score_base = 'final-superheavy'\n",
    "score_path = os.path.join(base_path, 'data', score_base)\n",
    "\n",
    "ddscore_df = pd.DataFrame(columns=['mean_count', 'frac_unscored', 'changed_score_mean', \n",
    "                                   'changed_score_std', 'score_mean', 'score_std',\n",
    "                                   'num_better', 'num_worse', 'num_unscored',\n",
    "                                   'mean_better', 'std_better',\n",
    "                                   'mean_worse', 'std_worse'])\n",
    "\n",
    "for pp_method in pp_methods:\n",
    "    ddscore_df.loc[pp_method] = score_pp_sb(pp_method, score_path, tol=50, crop=20)\n",
    "\n",
    "print(score_base)\n",
    "ddscore_df.sort_values(by='changed_score_mean', ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NewHomographyFinder\n",
    "\n",
    "Trying to get alignments without DeepMatchiing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = np.identity(3)\n",
    "img_1_h = cv.warpPerspective(img_1, h, dsize=img_1.shape)\n",
    "\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_from_h_matrix(img_1, img_2, h):\n",
    "    img_1_h = cv.warpPerspective(img_1, h, dsize=img_1.shape)\n",
    "    score = dd_score(img_1[10:,:490], img_1_h[10:,:490], img_2[10:,:490])\n",
    "    return score\n",
    "\n",
    "def get_gradient(img_1, img_2, h, param_i, epsilon):\n",
    "    h = h.reshape(-1,)\n",
    "    \n",
    "    h_up = h.copy()\n",
    "    h_up[param_i] += epsilon\n",
    "    \n",
    "    h_down = h.copy()\n",
    "    h_down[param_i] -= epsilon\n",
    "    \n",
    "    score_up = score_from_h_matrix(img_1, img_2, h_up.reshape(3,3))\n",
    "    score_down = score_from_h_matrix(img_1, img_2, h_down.reshape(3,3))\n",
    "    \n",
    "    grad = (score_up - score_down) / (2 * epsilon)\n",
    "    \n",
    "    return grad\n",
    "\n",
    "def get_all_gradients(img_1, img_2, h, epsilon):\n",
    "    grads = [get_gradient(img_1, img_2, h, i, epsilon)\n",
    "            for i in range(0,8)]\n",
    "    \n",
    "    return np.array(grads)\n",
    "        \n",
    "    \n",
    "def update_h_matrix(img_1, img_2, h, alpha, epsilon):\n",
    "    gradients = get_all_gradients(img_1, img_2, h, epsilon)\n",
    "    #print('grads')\n",
    "    #print(gradients)\n",
    "    if np.all(gradients == 0):\n",
    "        print('zero grad')\n",
    "    deltas = gradients * alpha\n",
    "    \n",
    "    #min_max_scaler = preprocessing.StandardScaler((-1e1,1e2))\n",
    "    #deltas = min_max_scaler.fit_transform(deltas[:, np.newaxis])\n",
    "\n",
    "    deltas = np.append(deltas, [0])\n",
    "    #print('deltas')\n",
    "    #print(deltas)\n",
    "    if np.any(np.abs(deltas) > 1e-3):\n",
    "        print('deltas too large')\n",
    "    if np.all(deltas == 0):\n",
    "        print('deltas are 0')\n",
    "    h_new = h.reshape(-1,) + deltas\n",
    "    return h_new.reshape(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimise_homography(img_1, img_2, alpha=1e-14, epsilon=1e-6, steps=50):\n",
    "    \n",
    "    h = np.identity(3)\n",
    "\n",
    "    scores = []\n",
    "    images = []\n",
    "\n",
    "    for i in range(steps):\n",
    "        #print('h')\n",
    "        #print(h)\n",
    "        h_new = update_h_matrix(img_1, img_2, h, alpha, epsilon)\n",
    "        img_1_h = cv.warpPerspective(img_1, h_new, dsize=img_1.shape)\n",
    "        score = dd_score(img_1[10:,:490], img_1_h[10:,:490], img_2[10:,:490])\n",
    "        scores.append(score)\n",
    "        images.append(img_1_h)\n",
    "        h = h_new.copy()\n",
    "        \n",
    "    return scores, images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 100\n",
    "scores, images = optimise_homography(img_1, img_2, epsilon=1e-4, alpha=1e-7, steps=steps)\n",
    "plt.plot(range(steps), scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_index = np.array(scores).argmax()\n",
    "best_image = images[best_index]\n",
    "\n",
    "plt.figure(1, figsize=(17,9))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(f'Overlaid Images - {pair[0]} and {pair[1]}')\n",
    "plt.imshow(overlay(img_1, img_2))\n",
    "\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(f'Transformed Overlay - {pair[0]} and {pair[1]}')\n",
    "plt.imshow(overlay(best_image, img_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(17,9))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(f'Overlaid Images - {pair[0]} and {pair[1]}')\n",
    "plt.imshow(overlay(img_1, img_2))\n",
    "\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(f'Transformed Overlay - {pair[0]} and {pair[1]}')\n",
    "plt.imshow(overlay(images[-1], img_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 2000\n",
    "pair = img_pairs[12]\n",
    "\n",
    "pp_path = 'clustered_norm'\n",
    "\n",
    "img_1 = cv.imread(os.path.join(base_path, 'data', pp_path, str(pair[0]) + '.png'), cv.IMREAD_GRAYSCALE)\n",
    "img_2 = cv.imread(os.path.join(base_path, 'data', pp_path, str(pair[1]) + '.png'), cv.IMREAD_GRAYSCALE)\n",
    "\n",
    "scores, images = optimise_homography(img_1, img_2, epsilon=1e-6, alpha=1e-14, steps=steps)\n",
    "\n",
    "plt.plot(range(steps), scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_index = np.array(scores).argmax()\n",
    "best_image = images[best_index]\n",
    "\n",
    "plt.figure(1, figsize=(17,9))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(f'Overlaid Images - {pair[0]} and {pair[1]}')\n",
    "plt.imshow(overlay(img_1, img_2))\n",
    "\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(f'Transformed Overlay - {pair[0]} and {pair[1]}')\n",
    "plt.imshow(overlay(best_image, img_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(17,9))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(f'Overlaid Images - {pair[0]} and {pair[1]}')\n",
    "plt.imshow(overlay(img_1, img_2))\n",
    "\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(f'Transformed Overlay - {pair[0]} and {pair[1]}')\n",
    "plt.imshow(overlay(images[0], img_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradients are a bit too crazy, will try a genetic algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "genetic algoritms to optimise the h matrix without using gradients (as they look unstable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "208px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
